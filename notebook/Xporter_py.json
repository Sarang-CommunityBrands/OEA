{
	"name": "Xporter_py",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "spark3p1sm",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"runAsWorkspaceSystemIdentity": false,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "a7a7656a-d800-41d3-be3b-58b85291d320"
			}
		},
		"metadata": {
			"saveOutput": true,
			"synapse_widget": {
				"version": "0.1"
			},
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/79a9a1b8-17c9-4098-bbae-16c21b7edd2a/resourceGroups/gdat-oea-uat/providers/Microsoft.Synapse/workspaces/gdat-oea-uat-syn/bigDataPools/spark3p1sm",
				"name": "spark3p1sm",
				"type": "Spark",
				"endpoint": "https://gdat-oea-uat-syn.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark3p1sm",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net",
					"authHeader": null
				},
				"sparkVersion": "3.1",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"extraHeader": null
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql.functions import col\r\n",
					"\"\"\"\r\n",
					"Provides data processing methods for Community Brands Xporter data.\r\n",
					"Data is expected to be received via Xporter into stage1np/xporter\r\n",
					"The structure of the folders in stage1np will then be something like:\r\n",
					"    -> stage1np/xporter/[EstabId]\r\n",
					"        -> stage1np/xporter/[EstabId]/Schoolinfo.json\r\n",
					"        -> stage1np/xporter/[EstabId]/Students.json\r\n",
					"        etc\r\n",
					"In stage2, everything is written to stage2np/xporter and stage2p/xporter\r\n",
					"\"\"\"\r\n",
					"class Xporter(BaseOEAModule):\r\n",
					"    def __init__(self, source_folder='xporter', pseudonymize = True):\r\n",
					"        BaseOEAModule.__init__(self, source_folder, pseudonymize)\r\n",
					"        self.schemas['schoolinfo'] = [\r\n",
					"                                        ['CurrentAcademicYear', 'string', 'no-op'],\r\n",
					"                                        ['DeniNo', 'string', 'no-op'],\r\n",
					"                                        ['Email', 'string', 'no-op'],\r\n",
					"                                        ['EstabId', 'string', 'no-op'],\r\n",
					"                                        ['ExamCentre', 'string', 'no-op'],\r\n",
					"                                        ['Governance', 'string', 'no-op'],\r\n",
					"                                        ['Head', 'string', 'no-op'],\r\n",
					"                                        ['Id', 'string', 'no-op'],\r\n",
					"                                        ['LastUpdated', 'string', 'no-op'],\r\n",
					"                                        ['MainContact', 'string', 'no-op'],\r\n",
					"                                        ['Name', 'string', 'no-op'],\r\n",
					"                                        ['Phase', 'string', 'no-op'],\r\n",
					"                                        ['RowHash', 'string', 'mask'],\r\n",
					"                                        ['SchoolLogoAlternateUrl', 'string', 'no-op'],\r\n",
					"                                        ['SchoolLogoUrl', 'string', 'no-op'],\r\n",
					"                                        ['Telephone', 'string', 'no-op'],\r\n",
					"                                        ['Web', 'string', 'no-op'],\r\n",
					"                                        ['SchoolID', 'string', 'partition_by'],\r\n",
					"                                        ['Address', 'string', 'no-op']]\r\n",
					"\r\n",
					"        self.schemas['students'] = [    ['AdmissionNo', 'string', 'no-op'],\r\n",
					"                                        ['Apartment', 'string', 'no-op'],\r\n",
					"                                        ['AsylumStatus', 'string', 'no-op'],\r\n",
					"                                        ['AttMarksEndDate', 'string', 'no-op'],\r\n",
					"                                        ['AttMarksStartDate', 'string', 'no-op'],\r\n",
					"                                        ['Boarder', 'string', 'no-op'],\r\n",
					"                                        ['BoardingHouse', 'string', 'no-op'],\r\n",
					"                                        ['CandidateNo', 'string', 'no-op'],\r\n",
					"                                        ['Country', 'string', 'no-op'],\r\n",
					"                                        ['CountryOfBirth', 'string', 'no-op'],\r\n",
					"                                        ['CountryOfBirthCode', 'string', 'no-op'],\r\n",
					"                                        ['County', 'string', 'no-op'],\r\n",
					"                                        ['DateofBirth', 'string', 'mask'],\r\n",
					"                                        ['Destination', 'string', 'no-op'],\r\n",
					"                                        ['DestinationStartDate', 'string', 'no-op'],\r\n",
					"                                        ['Disabled', 'string', 'no-op'],\r\n",
					"                                        ['DisplayName', 'string', 'no-op'],\r\n",
					"                                        ['District', 'string', 'no-op'],\r\n",
					"                                        ['EAL', 'string', 'no-op'],\r\n",
					"                                        ['EnglishProficiencyLevel', 'string', 'no-op'],\r\n",
					"                                        ['EnglishProficiencyLevelCode', 'string', 'no-op'],\r\n",
					"                                        ['EnrolmentStatus', 'string', 'no-op'],\r\n",
					"                                        ['EntryDate', 'string', 'no-op'],\r\n",
					"                                        ['Ethnicity', 'string', 'no-op'],\r\n",
					"                                        ['EthnicityCode', 'string', 'no-op'],\r\n",
					"                                        ['EthnicitySource', 'string', 'no-op'],\r\n",
					"                                        ['EverInCare', 'string', 'no-op'],\r\n",
					"                                        ['ExternalId', 'string', 'no-op'],\r\n",
					"                                        ['FSMEver6', 'string', 'no-op'],\r\n",
					"                                        ['FirstLanguage', 'string', 'no-op'],\r\n",
					"                                        ['FirstLanguageCode', 'string', 'no-op'],\r\n",
					"                                        ['FirstLanguageSource', 'string', 'no-op'],\r\n",
					"                                        ['Forename', 'string', 'no-op'],\r\n",
					"                                        ['FsmEligible', 'string', 'no-op'],\r\n",
					"                                        ['FsmStartDate', 'string', 'no-op'],\r\n",
					"                                        ['FsmEndDate', 'string', 'no-op'],\r\n",
					"                                        ['Gender', 'string', 'no-op'],\r\n",
					"                                        ['Gifted', 'string', 'no-op'],\r\n",
					"                                        ['HomeLanguage', 'string', 'no-op'],\r\n",
					"                                        ['HomeLanguageCode', 'string', 'no-op'],\r\n",
					"                                        ['HouseGroup', 'string', 'no-op'],\r\n",
					"                                        ['HouseGroupId', 'string', 'no-op'],\r\n",
					"                                        ['HouseName', 'string', 'no-op'],\r\n",
					"                                        ['HouseNo', 'string', 'no-op'],\r\n",
					"                                        ['Id', 'string', 'no-op'],\r\n",
					"                                        ['IdaasEmail', 'string', 'no-op'],\r\n",
					"                                        ['IdaasId', 'string', 'no-op'],\r\n",
					"                                        ['InLeaCare', 'string', 'no-op'],\r\n",
					"                                        ['IsTraveller', 'string', 'no-op'],\r\n",
					"                                        ['IsYoungCarer', 'string', 'no-op'],\r\n",
					"                                        ['KeyStage', 'string', 'no-op'],\r\n",
					"                                        ['LastUpdated', 'string', 'no-op'],\r\n",
					"                                        ['LeaCareAuthority', 'string', 'no-op'],\r\n",
					"                                        ['LeavingDate', 'string', 'no-op'],\r\n",
					"                                        ['LeavingRegGroup', 'string', 'no-op'],\r\n",
					"                                        ['LeavingYearGroup', 'string', 'no-op'],\r\n",
					"                                        ['LegalForename', 'string', 'no-op'],\r\n",
					"                                        ['LegalSurname', 'string', 'no-op'],\r\n",
					"                                        ['Marks', 'string', 'no-op'],\r\n",
					"                                        ['MiddleName', 'string', 'no-op'],\r\n",
					"                                        ['ModeOfTravel', 'string', 'no-op'],\r\n",
					"                                        ['NCYear', 'string', 'no-op'],\r\n",
					"                                        ['NationalIdentity', 'string', 'no-op'],\r\n",
					"                                        ['Nationality', 'string', 'no-op'],\r\n",
					"                                        ['OnReport', 'string', 'no-op'],\r\n",
					"                                        ['ParentalSalutation', 'string', 'no-op'],\r\n",
					"                                        ['PartTime', 'string', 'no-op'],\r\n",
					"                                        ['PostCode', 'string', 'no-op'],\r\n",
					"                                        ['PreviousLegalSurname', 'string', 'no-op'],\r\n",
					"                                        ['PupilPremium', 'string', 'no-op'],\r\n",
					"                                        ['QuickNote', 'string', 'no-op'],\r\n",
					"                                        ['ReasonForLeaving', 'string', 'no-op'],\r\n",
					"                                        ['RegGroup', 'string', 'no-op'],\r\n",
					"                                        ['RegGroupId', 'string', 'no-op'],\r\n",
					"                                        ['Religion', 'string', 'no-op'],\r\n",
					"                                        ['ReligionCode', 'string', 'no-op'],\r\n",
					"                                        ['RowHash', 'string', 'no-op'],\r\n",
					"                                        ['SENProvision', 'string', 'no-op'],\r\n",
					"                                        ['ServiceChild', 'string', 'no-op'],\r\n",
					"                                        ['ServiceChildSource', 'string', 'no-op'],\r\n",
					"                                        ['StandardYearGroupCode', 'string', 'no-op'],\r\n",
					"                                        ['StandardYearGroupName', 'string', 'no-op'],\r\n",
					"                                        ['Street', 'string', 'no-op'],\r\n",
					"                                        ['StudentStatus', 'string', 'no-op'],\r\n",
					"                                        ['Surname', 'string', 'no-op'],\r\n",
					"                                        ['TownOrCity', 'string', 'no-op'],\r\n",
					"                                        ['TravellerSource', 'string', 'no-op'],\r\n",
					"                                        ['UPN', 'string', 'no-op'],\r\n",
					"                                        ['UniformAllowance', 'string', 'no-op'],\r\n",
					"                                        ['UniqueLearnerNumber', 'string', 'no-op'],\r\n",
					"                                        ['WorkEmail', 'string', 'no-op'],\r\n",
					"                                        ['XID', 'string', 'no-op'],\r\n",
					"                                        ['YSSA', 'string', 'no-op'],\r\n",
					"                                        ['YearGroup', 'string', 'no-op'],\r\n",
					"                                        ['YearGroupId', 'string', 'no-op'],\r\n",
					"                                        ['YearTaughtIn', 'string', 'no-op'],\r\n",
					"                                        ['formerUPN', 'string', 'no-op'],\r\n",
					"                                        ['SchoolID', 'string', 'partition_by'],\r\n",
					"                                        ['AddressBlock', 'string', 'no-op']\r\n",
					"                                        ] \r\n",
					"\r\n",
					"\r\n",
					"        self.schemas['attendancesummary'] = [['Id', 'string', 'no-op'],\r\n",
					"                                            ['XID', 'string', 'no-op'],\r\n",
					"                                            ['MIS_ID', 'string', 'no-op'],\r\n",
					"                                            ['IdaasId', 'string', 'no-op'],\r\n",
					"                                            ['AttStatsStartDate', 'string', 'no-op'],\r\n",
					"                                            ['AttStatsEndDate', 'string', 'no-op'],\r\n",
					"                                            ['NumPossMarks', 'string', 'no-op'],\r\n",
					"                                            ['NumPresMarks', 'string', 'no-op'],\r\n",
					"                                            ['NumAEAMarks', 'string', 'no-op'],\r\n",
					"                                            ['NumAuthAbsMarks', 'string', 'no-op'],\r\n",
					"                                            ['NumUnauthAbsMarks', 'string', 'no-op'],\r\n",
					"                                            ['NumMissMarks', 'string', 'no-op'],\r\n",
					"                                            ['NumLateMarks', 'string', 'no-op'],\r\n",
					"                                            ['NumLateBeforeRegMarks', 'string', 'no-op'],\r\n",
					"                                            ['Marks', 'string', 'mask'],\r\n",
					"                                            ['SchoolID', 'string', 'partition_by']] \r\n",
					"\r\n",
					"        self.schemas['groups'] = [['Code', 'string', 'no-op'],\r\n",
					"                                ['ExternalId', 'string', 'no-op'],\r\n",
					"                                ['Id', 'string', 'no-op'],\r\n",
					"                                ['IdaasId', 'string', 'no-op'],\r\n",
					"                                ['LastUpdated', 'string', 'no-op'],\r\n",
					"                                ['Name', 'string', 'no-op'],\r\n",
					"                                ['NumStudents', 'string', 'no-op'],\r\n",
					"                                ['PlatformId', 'string', 'no-op'],\r\n",
					"                                ['PrimaryStaffId', 'string', 'no-op'],\r\n",
					"                                ['RowHash', 'string', 'mask'],\r\n",
					"                                ['Staff', 'string', 'no-op'],\r\n",
					"                                ['Type', 'string', 'no-op'],\r\n",
					"                                ['XID', 'string', 'no-op'],\r\n",
					"                                ['SchoolID', 'string', 'partition_by']] \r\n",
					"\r\n",
					"        self.schemas['HistoricalAttendanceSummary'] = [['EndDate', 'string', 'no-op'],\r\n",
					"                                                ['Id','string', 'no-op'],\r\n",
					"                                                ['SchoolYear', 'string', 'no-op'],\r\n",
					"                                                ['StartDate', 'string', 'no-op'],\r\n",
					"                                                ['StudentId', 'string', 'mask'],\r\n",
					"                                                ['SchoolID', 'string', 'partition_by'],\r\n",
					"                                                ['Marks', 'string', 'no-op']]\r\n",
					"\r\n",
					"        self.schemas['staff'] = [[\"Apartment\",\"string\", 'no-op'],\r\n",
					"                                [\"Country\",\"string\", 'no-op'],\r\n",
					"                                [\"County\",\"string\", 'no-op'],\r\n",
					"                                [\"DateOfBirth\", \"string\", 'no-op'],\r\n",
					"                                [\"DisplayName\", 'string', 'no-op'],\r\n",
					"                                [\"District\",\"string\", 'no-op'],\r\n",
					"                                [\"EmploymentEnd\",\"string\", 'no-op'],\r\n",
					"                                [\"EmploymentStart\", 'string', 'no-op'],\r\n",
					"                                [\"ExternalId\",\"string\", 'no-op'],\r\n",
					"                                [\"Forename\",\"string\", 'no-op'],\r\n",
					"                                [\"Gender\",\"string\", 'no-op'],\r\n",
					"                                [\"HomeEmail\", 'string', 'no-op'],\r\n",
					"                                [\"HomePhone\", 'string', 'no-op'],\r\n",
					"                                [\"HouseName\", 'string', 'no-op'],\r\n",
					"                                [\"HouseNo\", 'string', 'no-op'],\r\n",
					"                                [\"Id\", 'string', 'no-op'],\r\n",
					"                                [\"IdaasEmail\", \"string\", 'no-op'],\r\n",
					"                                [\"IdaasId\", \"string\", 'no-op'],\r\n",
					"                                [\"IsSupply\", \"string\", 'no-op'],\r\n",
					"                                [\"IsSupport\", 'string', 'no-op'],\r\n",
					"                                [\"IsTeacher\", \"string\", 'no-op'],\r\n",
					"                                [\"LastUpdated\", \"string\", 'no-op'],\r\n",
					"                                [\"LegalForename\", \"string\", 'no-op'],\r\n",
					"                                [\"LegalSurname\", 'string', 'no-op'],\r\n",
					"                                [\"MiddleName\", \"string\", 'no-op'],\r\n",
					"                                [\"MobilePhone\", \"string\", 'no-op'],\r\n",
					"                                [\"NINumber\", \"string\", 'no-op'],\r\n",
					"                                [\"PayrollNumber\", \"string\", 'no-op'],\r\n",
					"                                [\"PostCode\", 'string', 'no-op'],\r\n",
					"                                [\"RegGroup\", \"string\", 'no-op'],\r\n",
					"                                [\"RoleCodes\", \"string\", 'no-op'],\r\n",
					"                                [\"Roles\", 'string', 'no-op'],\r\n",
					"                                [\"RowHash\", 'string', 'mask'],\r\n",
					"                                [\"StaffCode\", \"string\", 'no-op'],\r\n",
					"                                [\"StaffStatus\", 'string', 'no-op'],\r\n",
					"                                [\"Street\", \"string\", 'no-op'],\r\n",
					"                                [\"Suffix\", 'string', 'no-op'],\r\n",
					"                                [\"Surname\", \"string\", 'no-op'],\r\n",
					"                                [\"TeacherCategory\", \"string\", 'no-op'],\r\n",
					"                                [\"TeacherNumber\", 'string', 'no-op'],\r\n",
					"                                [\"Title\", 'string', 'no-op'],\r\n",
					"                                [\"TownOrCity\", 'string', 'no-op'],\r\n",
					"                                [\"WorkEmail\", 'string', 'no-op'],\r\n",
					"                                [\"WorkPhone\", \"string\", 'no-op'],\r\n",
					"                                [\"XID\", \"string\", 'no-op'],\r\n",
					"                                ['SchoolID', 'string', 'partition_by'],\r\n",
					"                                [\"AddressBlock\",'string', 'no-op']]\r\n",
					"\r\n",
					"\r\n",
					"        self.schemas['StudentMembers'] = [['EndDate', 'string', 'no-op'],\r\n",
					"                                        ['GroupExternalId', 'string', 'no-op'],\r\n",
					"                                        ['GroupId', 'string', 'no-op'],\r\n",
					"                                        ['GroupIdaasId', 'string', 'no-op'],\r\n",
					"                                        ['Id', 'string', 'no-op'],\r\n",
					"                                        ['LastUpdated', 'string', 'no-op'],\r\n",
					"                                        ['RowHash', 'string', 'mask'],\r\n",
					"                                        ['StartDate', 'string', 'no-op'],\r\n",
					"                                        ['StudentExternalId', 'string', 'no-op'],\r\n",
					"                                        ['StudentId', 'string', 'no-op'],\r\n",
					"                                        ['StudentIdaasId', 'string', 'no-op'],\r\n",
					"                                        ['SchoolID', 'string', 'partition_by']]\r\n",
					"                                        \r\n",
					" \r\n",
					"    \r\n",
					"    \"\"\"   def _prepare_schoolinfo(self):\r\n",
					"        # initialise dataframe and clear out any existing schoolinfo parquet\r\n",
					"        oea.rm_if_exists(oea.stage1np + '/xporter/schoolinfo/')\r\n",
					"        df_schoolinfo = None\r\n",
					"        # loop through school EstabId folders landed by Xporter and union schoolinfo\r\n",
					"        for folder in oea.get_folders(oea.stage1np + '/xporter'):\r\n",
					"            try:\r\n",
					"                new_df = oea.load(folder,'SchoolInfo.json',stage = oea.stage1np + '/xporter',data_format='json')\r\n",
					"                if df_schoolinfo is None:\r\n",
					"                    df_schoolinfo = new_df\r\n",
					"                else:\r\n",
					"                    df_schoolinfo = df_schoolinfo.union(new_df)\r\n",
					"            except:\r\n",
					"                pass\r\n",
					"        # explode from the json array to columns\r\n",
					"        df_schoolinfo = df_schoolinfo.select(F.explode('SchoolInfo').alias('exploded_values')).select(\"exploded_values.*\")\r\n",
					"        # write the aggregated parquet file out to load to stage 2\r\n",
					"        df_schoolinfo.write.parquet(oea.stage1np + '/xporter/schoolinfo')\"\"\"\r\n",
					"\r\n",
					"    def _prepare_schoolinfo(self):\r\n",
					"        from pyspark.sql.functions import lit\r\n",
					"        oea.rm_if_exists(oea.stage1np + '/xporter/schoolinfocsv')\r\n",
					"        df_schoolinfo = None\r\n",
					"        # loop through school EstabId folders landed by Xporter and union schoolinfo\r\n",
					"        for folder in oea.get_folders(oea.stage1np + '/xporter'):\r\n",
					"            if folder.isnumeric():\r\n",
					"                print(folder)\r\n",
					"                try:\r\n",
					"                    new_df = oea.load(folder,'SchoolInfo.json',stage = oea.stage1np + '/xporter',data_format='json')\r\n",
					"                    new_df = new_df.select(F.explode('SchoolInfo').alias('exploded_values')).select(\"exploded_values.*\")\r\n",
					"                    new_df = new_df.withColumn('SchoolID',lit(folder))\r\n",
					"                    if df_schoolinfo is None:\r\n",
					"                        df_schoolinfo = new_df\r\n",
					"                    else:\r\n",
					"                        df_schoolinfo = df_schoolinfo.union(new_df)\r\n",
					"                except:\r\n",
					"                    pass\r\n",
					"        print('newdf')\r\n",
					"        new_df.show()\r\n",
					"        print('shoolinfo')\r\n",
					"        df_schoolinfo.show()\r\n",
					"        # explode from the json array to columns\r\n",
					"        #df_schoolinfo1 = df_schoolinfo.select(F.explode('SchoolInfo').alias('exploded_values')).select(\"exploded_values.*\")\r\n",
					"\r\n",
					"        # write the aggregated csv combining all schoolinfo data file out to load to stage 1\r\n",
					"        df_schoolinfo.write.option(\"header\",\"true\").csv(oea.stage1np + '/xporter/schoolinfocsv')\r\n",
					"\r\n",
					"\r\n",
					"    \"\"\"def _prepare_schoolinfonew(self):\r\n",
					"        # initialise dataframe and clear out any existing schoolinfo parquet\r\n",
					"        oea.rm_if_exists(oea.stage1np + '/xporter/schoolinfo/')\r\n",
					"        df_schoolinfo = None\r\n",
					"        # loop through school EstabId folders landed by Xporter and union schoolinfo\r\n",
					"        for folder in oea.get_folders(oea.stage1np + '/xporter'):\r\n",
					"            try:\r\n",
					"                new_df = oea.load(folder,'SchoolInfo.json',stage = oea.stage1np + '/xporter',data_format='json')\r\n",
					"                if df_schoolinfo is None:\r\n",
					"                    df_schoolinfo = new_df\r\n",
					"                else:\r\n",
					"                    df_schoolinfo = df_schoolinfo.union(new_df)\r\n",
					"            except:\r\n",
					"                pass\r\n",
					"        # explode from the json array to columns\r\n",
					"        df_schoolinfo = df_schoolinfo.select(F.explode('SchoolInfo').alias('exploded_values')).select(\"exploded_values.*\")\r\n",
					"        # write the aggregated csv combining all schoolinfo data file out to load to stage 1\r\n",
					"        df_schoolinfo.write.option(\"header\",\"true\").csv(oea.stage1np + '/xporter/schoolinfo/schoolinfocsv.csv')\"\"\"\r\n",
					"\r\n",
					"\r\n",
					"    \"\"\"def _prepare_schoolinfonew_pipe(self):\r\n",
					"        # initialise dataframe and clear out any existing schoolinfo parquet\r\n",
					"        oea.rm_if_exists(oea.stage1np + '/xporter/schoolinfo/')\r\n",
					"        df_schoolinfo = None\r\n",
					"        # loop through school EstabId folders landed by Xporter and union schoolinfo\r\n",
					"        for folder in oea.get_folders(oea.stage1np + '/xporter'):\r\n",
					"            try:\r\n",
					"                new_df = oea.load(folder,'SchoolInfo.json',stage = oea.stage1np + '/xporter',data_format='json')\r\n",
					"                if df_schoolinfo is None:\r\n",
					"                    df_schoolinfo = new_df\r\n",
					"                else:\r\n",
					"                    df_schoolinfo = df_schoolinfo.union(new_df)\r\n",
					"            except:\r\n",
					"                pass\r\n",
					"        # explode from the json array to columns\r\n",
					"        df_schoolinfo = df_schoolinfo.select(F.explode('SchoolInfo').alias('exploded_values')).select(\"exploded_values.*\")\r\n",
					"        # write the aggregated csv combining all schoolinfo data file out to load to stage 1\r\n",
					"        df_schoolinfo.write.option(\"delimeter\",\"|\").option(\"header\",\"true\").csv(oea.stage1np + '/xporter/schoolinfo/schoolinfocsv.csv')\"\"\"\r\n",
					"        \r\n",
					"        \r\n",
					"    def ingest_schoolinfo(self):\r\n",
					"        # ingest delta data to stage 2 ingest_snapshot_data\r\n",
					"       #oea.ingest_delta_data('/xporter', '/schoolinfo', xporter.schemas['schoolinfo'], 'EstabId', primary_key='EstabId', data_format='parquet', has_header=True)\r\n",
					"       #self, source_system, tablename, schema, partition_by, primary_key='id', data_format='csv', has_header=True\r\n",
					"       #oea.ingest_snapshot_data('xporter', 'schoolinfo', xporter.schemas['schoolinfo'], 'EstabId')\r\n",
					"       #oea.ingest_snapshot_data('snapshot_example', 'student', example_schema, 'school_year')\r\n",
					"        csvd = oea.load_csv(\"stage1np/xporter/schoolinfocsv/*.csv\") #reading csv file\r\n",
					"        print(csvd)\r\n",
					"        csvd.show()\r\n",
					"\r\n",
					"        newdf = csvd[csvd['EstabId'] != 'null']\r\n",
					"        newdf = newdf.drop('Address')\r\n",
					"        #newdf = newdf.columns.append('Address')\r\n",
					"        newdf = newdf.withColumn(\"Address\",col(\"RowHash\"))\r\n",
					"        newdf.show()\r\n",
					"        oea.rm_if_exists(oea.stage1np + '/xporter/schoolinfo')#remove existing schoolinfo folder having old batch folder\r\n",
					"        oea.land(\"xporter\", \"schoolinfo\", newdf)#creating batch folder\r\n",
					"\r\n",
					"        oea.ingest_snapshot_data('xporter', 'schoolinfo', xporter.schemas['schoolinfo'], 'SchoolID', primary_key='EstabId')#ingesting into stage 2\r\n",
					"        #ingest_snapshot_data(self, source_system, tablename, schema, partition_by, primary_key='id', data_format='csv', has_header=True):\r\n",
					"\r\n",
					"\r\n",
					"    def ingest_schoolinfo_stage3(self):\r\n",
					"        \"\"\" Processes delta batch data from stage2 into stage3 \"\"\"\r\n",
					"        #oea.rm_if_exists(oea.stage3p + '/xporter/schoolinfo')\r\n",
					"        #oea.rm_if_exists(oea.stage3np + '/xporter/schoolinfo')   \r\n",
					"        source_path = f'{oea.stage2p}/xporter/schoolinfo_pseudo'\r\n",
					"        source_path2 = f'{oea.stage2np}/xporter/schoolinfo_lookup'\r\n",
					"        p_destination_path = f'{oea.stage3p}/xporter/schoolinfo_pseudo'\r\n",
					"        np_destination_path = f'{oea.stage3np}/xporter/schoolinfo_lookup'\r\n",
					"        spark_schema = oea.to_spark_schema(xporter.schemas['schoolinfo'])\r\n",
					"\r\n",
					"        df_1 = spark.read.load(source_path, format='parquet', header='true', schema=spark_schema)\r\n",
					"        df_2 = spark.read.load(source_path2, format='parquet', header='true', schema=spark_schema)\r\n",
					"        print(\"df_1\")\r\n",
					"        df_1.show()\r\n",
					"        print(\"df_2\")\r\n",
					"        df_2.show()\r\n",
					"        # df = df.dropDuplicates('EstabId') # More info: https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#streaming-deduplication\r\n",
					"        df2 = df_1.withColumnRenamed('Name','School_Name').withColumnRenamed('Head', 'Head_Teacher')\r\n",
					"        df_pseudo = df2.drop(col(\"Telephone\"))\r\n",
					"        df_lookup = df_2.drop(col(\"Telephone\"))\r\n",
					"        df_pseudo.write.save(p_destination_path, mode='overwrite', partitionBy='SchoolID')\r\n",
					"        df_lookup.write.save(np_destination_path, mode = 'overwrite', partitionBy = 'SchoolID')\r\n",
					"\r\n",
					"        #oea.ingest_snapshot_data_stage3('xporter', 'schoolinfo', xporter.schemas['schoolinfo'], 'SchoolID', primary_key='EstabId')\r\n",
					"    \r\n",
					"    \r\n",
					"    \"\"\"def ingest_schoolinfonew(self):\r\n",
					"        csvd = oea.load_csv(\"stage1np/xporter/schoolinfo/schoolinfocsv.csv\") #reading csv file\r\n",
					"        print(csvd)\r\n",
					"        #oea.land(\"xporter\", \"schoolinfo\", csvd)#creating batch folder\r\n",
					"        #oea.ingest_snapshot_data('xporter', 'schoolinfo', xporter.schemas['schoolinfo'], 'EstabId')#ingesting into stage 2\"\"\"\r\n",
					"\r\n",
					"\r\n",
					"    \"\"\"def _prepare_students(self):\r\n",
					"        # initialise dataframe and clear out any existing students parquet\r\n",
					"        oea.rm_if_exists(oea.stage1np + '/xporter/students/')\r\n",
					"        df_students = None\r\n",
					"        # loop through school EstabId folders landed by Xporter and union students\r\n",
					"        for folder in oea.get_folders(oea.stage1np + '/xporter'):\r\n",
					"            try:\r\n",
					"                new_df = oea.load(folder,'Students.json',stage = oea.stage1np + '/xporter',data_format='json')\r\n",
					"                if df_students is None:\r\n",
					"                    df_students = new_df\r\n",
					"                else:\r\n",
					"                    df_students = df_students.union(new_df)\r\n",
					"            except:\r\n",
					"                pass\r\n",
					"        # explode from the json array to columns\r\n",
					"        df_students = df_students.select(F.explode('Students').alias('exploded_values')).select(\"exploded_values.*\")\r\n",
					"        # write the aggregated parquet file out to load to stage 2\r\n",
					"        df_students.write.parquet(oea.stage1np + '/xporter/students')\"\"\"   \r\n",
					"        \r\n",
					"    \"\"\"def _prepare_studentsnew(self):\r\n",
					"        # initialise dataframe and clear out any existing students parquet\r\n",
					"        oea.rm_if_exists(oea.stage1np + '/xporter/students/')\r\n",
					"        df_students = None\r\n",
					"        # loop through school EstabId folders landed by Xporter and union students\r\n",
					"        for folder in oea.get_folders(oea.stage1np + '/xporter'):\r\n",
					"            try:\r\n",
					"                new_df = oea.load(folder,'Students.json',stage = oea.stage1np + '/xporter',data_format='json')\r\n",
					"                if df_students is None:\r\n",
					"                    df_students = new_df\r\n",
					"                else:\r\n",
					"                    df_students = df_students.union(new_df)\r\n",
					"            except:\r\n",
					"                pass\r\n",
					"        # explode from the json array to columns\r\n",
					"        df_students = df_students.select(F.explode('Students').alias('exploded_values')).select(\"exploded_values.*\")\r\n",
					"        # write the aggregated csv combining all students data file out to load to stage 1\r\n",
					"        df_students.write.option(\"header\",\"true\").csv(oea.stage1np + '/xporter/students/studentscsv.csv')\"\"\"\r\n",
					"    \r\n",
					"    \"\"\"def _prepare_studentsnew_pipe(self):\r\n",
					"        # initialise dataframe and clear out any existing students parquet\r\n",
					"        oea.rm_if_exists(oea.stage1np + '/xporter/students/')\r\n",
					"        df_students = None\r\n",
					"        # loop through school EstabId folders landed by Xporter and union students\r\n",
					"        for folder in oea.get_folders(oea.stage1np + '/xporter'):\r\n",
					"            try:\r\n",
					"                new_df = oea.load(folder,'Students.json',stage = oea.stage1np + '/xporter',data_format='json')\r\n",
					"                if df_students is None:\r\n",
					"                    df_students = new_df\r\n",
					"                else:\r\n",
					"                    df_students = df_students.union(new_df)\r\n",
					"            except:\r\n",
					"                pass\r\n",
					"        # explode from the json array to columns\r\n",
					"        df_students = df_students.select(F.explode('Students').alias('exploded_values')).select(\"exploded_values.*\")\r\n",
					"        # write the aggregated csv combining all students data file out to load to stage 1\r\n",
					"        df_students.write.option(\"header\",\"true\").option(\"delimiter\",\"|\").csv(oea.stage1np + '/xporter/students/studentscsv.csv')\"\"\"\r\n",
					"    \r\n",
					"    def _prepare_students(self):\r\n",
					"        from pyspark.sql.functions import lit\r\n",
					"        oea.rm_if_exists(oea.stage1np + '/xporter/studentscsv')\r\n",
					"        df_students = None\r\n",
					"        for folder in oea.get_folders(oea.stage1np + '/xporter'):\r\n",
					"            if folder.isnumeric():\r\n",
					"                try:\r\n",
					"                    new_df = oea.load(folder,'Students.json',stage = oea.stage1np + '/xporter',data_format='json')\r\n",
					"                    new_df = new_df.select(F.explode('Students').alias('exploded_values')).select(\"exploded_values.*\")\r\n",
					"                    new_df = new_df.withColumn('SchoolID',lit(folder))\r\n",
					"                    if df_students is None:\r\n",
					"                        df_students = new_df\r\n",
					"                    else:\r\n",
					"                        df_students = df_students.union(new_df)\r\n",
					"                except:\r\n",
					"                    pass\r\n",
					"        # explode from the json array to columns\r\n",
					"        #df_students = df_students.select(F.explode('Students').alias('exploded_values')).select(\"exploded_values.*\")\r\n",
					"        print('df_students - ')\r\n",
					"        df_students.show()\r\n",
					"        # write the aggregated csv combining all schoolinfo data file out to load to stage 1\r\n",
					"        df_students.write.option(\"header\",\"true\").csv(oea.stage1np + '/xporter/studentscsv')\r\n",
					"        print('df_students - ')\r\n",
					"        df_students.show()\r\n",
					"\r\n",
					"\r\n",
					"    def ingest_students(self):\r\n",
					"        # ingest delta data to stage 2    \r\n",
					"        #oea.ingest_snapshot_data('/xporter', '/students', xporter.schemas['students'], 'StudentStatus', primary_key='ExternalId', data_format='parquet', has_header=True)\r\n",
					"        #oea.ingest_delta_data('/xporter', '/students', xporter.schemas['students'], 'StudentStatus', primary_key='ExternalId', data_format='parquet', has_header=True)\r\n",
					"        csvd = oea.load_csv(\"stage1np/xporter/studentscsv/*.csv\") #reading csv file\r\n",
					"        print(csvd)\r\n",
					"        csvd.show()\r\n",
					"\r\n",
					"        newdf = csvd[csvd['StudentStatus'] != 'null']\r\n",
					"        newdf = newdf.drop('AddressBlock')\r\n",
					"\r\n",
					"        newdf = newdf.withColumn(\"AddressBlock\",col(\"Forename\"))\r\n",
					"        newdf.show()\r\n",
					"        oea.rm_if_exists(oea.stage1np + '/xporter/students')#remove existing students folder having old batch folder\r\n",
					"        oea.land(\"xporter\", \"students\", newdf)#creating batch folder\r\n",
					"\r\n",
					"        oea.ingest_snapshot_data('xporter', 'students', xporter.schemas['students'], 'SchoolID', primary_key='ExternalId')#ingesting into stage 2\r\n",
					"        #ingest_snapshot_data(self, source_system, tablename, schema, partition_by, primary_key='id', data_format='csv', has_header=True):\r\n",
					"\r\n",
					"    \"\"\"def ingest_studentsnew(self):\r\n",
					"        csvd = oea.load_csv_pipe(\"stage1np/xporter/students/studentscsv.csv\") #reading csv file\r\n",
					"        oea.land_pipe(\"xporter\", \"students\", csvd,partition_label='StudentStatus', format_str='csv')#creating batch folder\r\n",
					"        oea.ingest_snapshot_data_pipe('xporter', 'students', xporter.schemas['students'], 'StudentStatus')#ingesting into stage 2\"\"\"\r\n",
					"    \r\n",
					"    \r\n",
					"    def ingest_students_stage3(self):\r\n",
					"        \"\"\" Processes delta batch data from stage2 into stage3 \"\"\"\r\n",
					"        source_path = f'{oea.stage2p}/xporter/students_pseudo'\r\n",
					"        source_path2 = f'{oea.stage2np}/xporter/students_lookup'\r\n",
					"        p_destination_path = f'{oea.stage3p}/xporter/students_pseudo'\r\n",
					"        np_destination_path = f'{oea.stage3np}/xporter/students_lookup'\r\n",
					"        spark_schema = oea.to_spark_schema(xporter.schemas['students'])\r\n",
					"\r\n",
					"        df_1 = spark.read.load(source_path, format='parquet', header='true', schema=spark_schema)\r\n",
					"        df_2 = spark.read.load(source_path2, format='parquet', header='true', schema=spark_schema)\r\n",
					"        print(\"df_1\")\r\n",
					"        df_1.show()\r\n",
					"        print(\"df_2\")\r\n",
					"        df_2.show()\r\n",
					"        # df = df.dropDuplicates('EstabId') # More info: https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#streaming-deduplication\r\n",
					"        #df2 = df_1.withColumnRenamed('Name','School_Name').withColumnRenamed('Head', 'Head_Teacher')\r\n",
					"        df_pseudo = df_1\r\n",
					"        df_lookup = df_2\r\n",
					"        df_pseudo.write.save(p_destination_path, mode='overwrite', partitionBy='SchoolID')\r\n",
					"        df_lookup.write.save(np_destination_path, mode = 'overwrite', partitionBy = 'SchoolID')\r\n",
					"\r\n",
					"        #oea.ingest_snapshot_data_stage3('xporter', 'schoolinfo', xporter.schemas['schoolinfo'], 'SchoolID', primary_key='EstabId')\r\n",
					"\r\n",
					"\r\n",
					"    def _prepare_attendancesummary(self):\r\n",
					"        from pyspark.sql.functions import lit\r\n",
					"        oea.rm_if_exists(oea.stage1np + '/xporter/attendancesummarycsv')\r\n",
					"        df_attendancesummary = None\r\n",
					"        for folder in oea.get_folders(oea.stage1np + '/xporter'):\r\n",
					"            if folder.isnumeric():\r\n",
					"                try:\r\n",
					"                    new_df = oea.load(folder,'AttendanceSummary.json',stage = oea.stage1np + '/xporter',data_format='json')\r\n",
					"                    new_df = new_df.select(F.explode('AttendanceSummary').alias('exploded_values')).select(\"exploded_values.*\")\r\n",
					"                    new_df = new_df.withColumn('SchoolID',lit(folder))\r\n",
					"                    if df_attendancesummary is None:\r\n",
					"                        df_attendancesummary = new_df\r\n",
					"                    else:\r\n",
					"                        df_attendancesummary = df_attendancesummary.union(new_df)\r\n",
					"                except:\r\n",
					"                    pass\r\n",
					"        print('new_df')\r\n",
					"        new_df.show()\r\n",
					"        # explode from the json array to columns\r\n",
					"        #df_attendancesummary = df_attendancesummary.select(F.explode('AttendanceSummary').alias('exploded_values')).select(\"exploded_values.*\")\r\n",
					"        # write the aggregated csv combining all schoolinfo data file out to load to stage 1\r\n",
					"        df_attendancesummary.write.option(\"header\",\"true\").csv(oea.stage1np + '/xporter/attendancesummarycsv')\r\n",
					"        print('df_attendancesummary - ')\r\n",
					"        df_attendancesummary.show()\r\n",
					"\r\n",
					"    def ingest_attendancesummary(self):\r\n",
					"        csvd = oea.load_csv(\"stage1np/xporter/attendancesummarycsv/*.csv\") #reading csv file\r\n",
					"        print(csvd)\r\n",
					"        oea.rm_if_exists(oea.stage1np + '/xporter/attendancesummary')\r\n",
					"        oea.land(\"xporter\", \"attendancesummary\", csvd)#creating batch folder\r\n",
					"        oea.ingest_snapshot_data('xporter', 'attendancesummary', xporter.schemas['attendancesummary'], 'SchoolID', primary_key='IdaasId')#ingesting into stage 2\"\"\"\r\n",
					"    \r\n",
					"    def ingest_attendancesummary_stage3(self):\r\n",
					"        \"\"\" Processes delta batch data from stage2 into stage3 \"\"\"\r\n",
					"        source_path = f'{oea.stage2p}/xporter/attendancesummary_pseudo'\r\n",
					"        source_path2 = f'{oea.stage2np}/xporter/attendancesummary_lookup'\r\n",
					"        p_destination_path = f'{oea.stage3p}/xporter/attendancesummary_pseudo'\r\n",
					"        np_destination_path = f'{oea.stage3np}/xporter/attendancesummary_lookup'\r\n",
					"        spark_schema = oea.to_spark_schema(xporter.schemas['attendancesummary'])\r\n",
					"\r\n",
					"        df_1 = spark.read.load(source_path, format='parquet', header='true', schema=spark_schema)\r\n",
					"        df_2 = spark.read.load(source_path2, format='parquet', header='true', schema=spark_schema)\r\n",
					"        print(\"df_1\")\r\n",
					"        df_1.show()\r\n",
					"        print(\"df_2\")\r\n",
					"        df_2.show()\r\n",
					"        # df = df.dropDuplicates('EstabId') # More info: https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#streaming-deduplication\r\n",
					"        #df2 = df_1.withColumnRenamed('Name','School_Name').withColumnRenamed('Head', 'Head_Teacher')\r\n",
					"        df_pseudo = df_1\r\n",
					"        df_lookup = df_2\r\n",
					"        df_pseudo.write.save(p_destination_path, mode='overwrite', partitionBy='SchoolID')\r\n",
					"        df_lookup.write.save(np_destination_path, mode = 'overwrite', partitionBy = 'SchoolID')\r\n",
					"\r\n",
					"        #oea.ingest_snapshot_data_stage3('xporter', 'schoolinfo', xporter.schemas['schoolinfo'], 'SchoolID', primary_key='EstabId')\r\n",
					"\r\n",
					"    \r\n",
					"    def _prepare_groups(self):\r\n",
					"        from pyspark.sql.functions import lit\r\n",
					"        oea.rm_if_exists(oea.stage1np + '/xporter/groupscsv')\r\n",
					"        df_groups1 = None\r\n",
					"        for folder in oea.get_folders(oea.stage1np + '/xporter'):\r\n",
					"            print(folder)\r\n",
					"            if folder.isnumeric():\r\n",
					"                try:\r\n",
					"                    new_df = oea.load(folder,'groups.json',stage = oea.stage1np + '/xporter',data_format='json')\r\n",
					"                    new_df = new_df.select(F.explode('Group').alias('exploded_values')).select(\"exploded_values.*\")\r\n",
					"                    new_df = new_df.withColumn('SchoolID',lit(folder))            \r\n",
					"                    if df_groups1 is None:\r\n",
					"                        df_groups1 = new_df\r\n",
					"                    else:\r\n",
					"                        df_groups1 = df_groups1.union(new_df)\r\n",
					"                except:\r\n",
					"                    pass\r\n",
					"        print('new_df3')\r\n",
					"        new_df.show()\r\n",
					"        df_groups1.write.option(\"header\",\"true\").csv(oea.stage1np + '/xporter/groupscsv')\r\n",
					"        print('df_groups1 - ')\r\n",
					"        df_groups1.show()\r\n",
					"        \r\n",
					"\r\n",
					"\r\n",
					"    def ingest_groups(self):\r\n",
					"        csvd = oea.load_csv(\"stage1np/xporter/groupscsv/*.csv\") #reading csv file\r\n",
					"        print(\"csvd\")\r\n",
					"        csvd.show()\r\n",
					"        oea.rm_if_exists(oea.stage1np + '/xporter/Group')\r\n",
					"        oea.land(\"xporter\", \"Group\", csvd)#creating batch folder\r\n",
					"        oea.ingest_snapshot_data('xporter', 'Group', xporter.schemas['groups'], 'SchoolID', primary_key='Id')#ingesting into stage 2\"\"\"\r\n",
					"\r\n",
					"    \r\n",
					"    def ingest_groups_stage3(self):\r\n",
					"        \"\"\" Processes delta batch data from stage2 into stage3 \"\"\"\r\n",
					"        source_path = f'{oea.stage2p}/xporter/Group_pseudo'\r\n",
					"        source_path2 = f'{oea.stage2np}/xporter/Group_lookup'\r\n",
					"        p_destination_path = f'{oea.stage3p}/xporter/Group_pseudo'\r\n",
					"        np_destination_path = f'{oea.stage3np}/xporter/Group_lookup'\r\n",
					"        spark_schema = oea.to_spark_schema(xporter.schemas['groups'])\r\n",
					"\r\n",
					"        df_1 = spark.read.load(source_path, format='parquet', header='true', schema=spark_schema)\r\n",
					"        df_2 = spark.read.load(source_path2, format='parquet', header='true', schema=spark_schema)\r\n",
					"        print(\"df_1\")\r\n",
					"        df_1.show()\r\n",
					"        print(\"df_2\")\r\n",
					"        df_2.show()\r\n",
					"        # df = df.dropDuplicates('EstabId') # More info: https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#streaming-deduplication\r\n",
					"        #df2 = df_1.withColumnRenamed('Name','School_Name').withColumnRenamed('Head', 'Head_Teacher')\r\n",
					"        df_pseudo = df_1\r\n",
					"        df_lookup = df_2\r\n",
					"        df_pseudo.write.save(p_destination_path, mode='overwrite', partitionBy='SchoolID')\r\n",
					"        df_lookup.write.save(np_destination_path, mode = 'overwrite', partitionBy = 'SchoolID')\r\n",
					"\r\n",
					"        #oea.ingest_snapshot_data_stage3('xporter', 'schoolinfo', xporter.schemas['schoolinfo'], 'SchoolID', primary_key='EstabId')\r\n",
					"\r\n",
					"    \r\n",
					"    def _prepare_HistoricalAttendanceSummary(self):\r\n",
					"        from pyspark.sql.functions import lit\r\n",
					"        oea.rm_if_exists(oea.stage1np + '/xporter/HistoricalAttendanceSummarycsv')\r\n",
					"        df_HistoricalAttendanceSummary = None\r\n",
					"        for folder in oea.get_folders(oea.stage1np + '/xporter'):\r\n",
					"            print(folder)\r\n",
					"            if folder.isnumeric():\r\n",
					"                try:\r\n",
					"                    new_df = oea.load(folder,'HistoricalAttendanceSummary.json',stage = oea.stage1np + '/xporter',data_format='json')\r\n",
					"                    new_df = new_df.select(F.explode('HistoricalAttendanceSummary').alias('exploded_values')).select(\"exploded_values.*\")\r\n",
					"                    new_df = new_df.withColumn('SchoolID',lit(folder))            \r\n",
					"                    if df_HistoricalAttendanceSummary is None:\r\n",
					"                        df_HistoricalAttendanceSummary = new_df\r\n",
					"                    else:\r\n",
					"                        df_HistoricalAttendanceSummary = df_HistoricalAttendanceSummary.union(new_df)\r\n",
					"                except:\r\n",
					"                    pass\r\n",
					"        print('new_df3')\r\n",
					"        new_df.show()\r\n",
					"        df_HistoricalAttendanceSummary.write.option(\"header\",\"true\").csv(oea.stage1np + '/xporter/HistoricalAttendanceSummarycsv')\r\n",
					"        print('df_groups1 - ')\r\n",
					"        df_HistoricalAttendanceSummary.show()\r\n",
					"\r\n",
					"\r\n",
					"    def ingest_HistoricalAttendanceSummary(self):\r\n",
					"        csvd = oea.load_csv(\"stage1np/xporter/HistoricalAttendanceSummarycsv/*.csv\") #reading csv file\r\n",
					"        print(\"csvd\")\r\n",
					"        csvd.show()\r\n",
					"        newdf = csvd[csvd['StudentId'] != 'null']\r\n",
					"        newdf = newdf.drop('Marks')\r\n",
					"        newdf = newdf.withColumn(\"Marks\",col(\"Id\"))\r\n",
					"        print('newdf')\r\n",
					"        newdf.show()\r\n",
					"        oea.rm_if_exists(oea.stage1np + '/xporter/HistoricalAttendanceSummary')\r\n",
					"        oea.land(\"xporter\", \"HistoricalAttendanceSummary\", newdf)#creating batch folder\r\n",
					"        oea.ingest_snapshot_data('xporter', 'HistoricalAttendanceSummary', xporter.schemas['HistoricalAttendanceSummary'], 'SchoolID', primary_key = 'Id')#ingesting into stage 2\"\"\"\r\n",
					"    \r\n",
					"    \r\n",
					"    def ingest_HistoricalAttendanceSummary_stage3(self):\r\n",
					"        \"\"\" Processes delta batch data from stage2 into stage3 \"\"\"\r\n",
					"        source_path = f'{oea.stage2p}/xporter/HistoricalAttendanceSummary_pseudo'\r\n",
					"        source_path2 = f'{oea.stage2np}/xporter/HistoricalAttendanceSummary_lookup'\r\n",
					"        p_destination_path = f'{oea.stage3p}/xporter/HistoricalAttendanceSummary_pseudo'\r\n",
					"        np_destination_path = f'{oea.stage3np}/xporter/HistoricalAttendanceSummary_lookup'\r\n",
					"        spark_schema = oea.to_spark_schema(xporter.schemas['HistoricalAttendanceSummary'])\r\n",
					"\r\n",
					"        df_1 = spark.read.load(source_path, format='parquet', header='true', schema=spark_schema)\r\n",
					"        df_2 = spark.read.load(source_path2, format='parquet', header='true', schema=spark_schema)\r\n",
					"        print(\"df_1\")\r\n",
					"        df_1.show()\r\n",
					"        print(\"df_2\")\r\n",
					"        df_2.show()\r\n",
					"        # df = df.dropDuplicates('EstabId') # More info: https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#streaming-deduplication\r\n",
					"        #df2 = df_1.withColumnRenamed('Name','School_Name').withColumnRenamed('Head', 'Head_Teacher')\r\n",
					"        df_pseudo = df_1\r\n",
					"        df_lookup = df_2\r\n",
					"        df_pseudo.write.save(p_destination_path, mode='overwrite', partitionBy='SchoolID')\r\n",
					"        df_lookup.write.save(np_destination_path, mode = 'overwrite', partitionBy = 'SchoolID')\r\n",
					"\r\n",
					"        #oea.ingest_snapshot_data_stage3('xporter', 'schoolinfo', xporter.schemas['schoolinfo'], 'SchoolID', primary_key='EstabId')\r\n",
					"   \r\n",
					"\r\n",
					"    def _prepare_staff(self):\r\n",
					"        from pyspark.sql.functions import lit\r\n",
					"        oea.rm_if_exists(oea.stage1np + '/xporter/staffcsv')\r\n",
					"        df_staff = None\r\n",
					"        for folder in oea.get_folders(oea.stage1np + '/xporter'):\r\n",
					"            print(folder)\r\n",
					"            if folder.isnumeric():\r\n",
					"                try:\r\n",
					"                    new_df = oea.load(folder,'staff.json',stage = oea.stage1np + '/xporter',data_format='json')\r\n",
					"                    new_df = new_df.select(F.explode('staff').alias('exploded_values')).select(\"exploded_values.*\")\r\n",
					"                    new_df = new_df.withColumn('SchoolID',lit(folder))            \r\n",
					"                    if df_staff is None:\r\n",
					"                        df_staff = new_df\r\n",
					"                    else:\r\n",
					"                        df_staff = df_staff.union(new_df)\r\n",
					"                except:\r\n",
					"                    pass\r\n",
					"        print('new_df3')\r\n",
					"        new_df.show()\r\n",
					"        df_staff.write.option(\"header\",\"true\").csv(oea.stage1np + '/xporter/staffcsv')\r\n",
					"        print('df_groups1 - ')\r\n",
					"        df_staff.show()\r\n",
					"\r\n",
					"    \r\n",
					"    def ingest_staff(self):\r\n",
					"        csvd = oea.load_csv(\"stage1np/xporter/staffcsv/*.csv\") #reading csv file\r\n",
					"        print(\"csvd\")\r\n",
					"        csvd.show()\r\n",
					"        newdf = csvd[csvd[\"ExternalId\"] != 'null']\r\n",
					"        newdf = newdf.drop('AddressBlock')\r\n",
					"        newdf = newdf.withColumn(\"AddressBlock\",col(\"Forename\"))\r\n",
					"        oea.rm_if_exists(oea.stage1np + '/xporter/staff')\r\n",
					"        oea.land(\"xporter\", \"staff\", newdf)#creating batch folder\r\n",
					"        oea.ingest_snapshot_data('xporter', 'staff', xporter.schemas['staff'], 'SchoolID', primary_key='Id')#ingesting into stage 2\"\"\"\r\n",
					"    \r\n",
					"    \r\n",
					"    def ingest_staff_stage3(self):\r\n",
					"        \"\"\" Processes delta batch data from stage2 into stage3 \"\"\"\r\n",
					"        source_path = f'{oea.stage2p}/xporter/staff_pseudo'\r\n",
					"        source_path2 = f'{oea.stage2np}/xporter/staff_lookup'\r\n",
					"        p_destination_path = f'{oea.stage3p}/xporter/staff_pseudo'\r\n",
					"        np_destination_path = f'{oea.stage3np}/xporter/staff_lookup'\r\n",
					"        spark_schema = oea.to_spark_schema(xporter.schemas['staff'])\r\n",
					"\r\n",
					"        df_1 = spark.read.load(source_path, format='parquet', header='true', schema=spark_schema)\r\n",
					"        df_2 = spark.read.load(source_path2, format='parquet', header='true', schema=spark_schema)\r\n",
					"        print(\"df_1\")\r\n",
					"        df_1.show()\r\n",
					"        print(\"df_2\")\r\n",
					"        df_2.show()\r\n",
					"        # df = df.dropDuplicates('EstabId') # More info: https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#streaming-deduplication\r\n",
					"        #df2 = df_1.withColumnRenamed('Name','School_Name').withColumnRenamed('Head', 'Head_Teacher')\r\n",
					"        df_pseudo = df_1\r\n",
					"        df_lookup = df_2\r\n",
					"        df_pseudo.write.save(p_destination_path, mode='overwrite', partitionBy='SchoolID')\r\n",
					"        df_lookup.write.save(np_destination_path, mode = 'overwrite', partitionBy = 'SchoolID')\r\n",
					"\r\n",
					"        #oea.ingest_snapshot_data_stage3('xporter', 'schoolinfo', xporter.schemas['schoolinfo'], 'SchoolID', primary_key='EstabId')\r\n",
					"\r\n",
					"    \r\n",
					"    def _prepare_StudentMembers(self):\r\n",
					"        from pyspark.sql.functions import lit\r\n",
					"        oea.rm_if_exists(oea.stage1np + '/xporter/StudentMemberscsv')\r\n",
					"        df_StudentMembers = None\r\n",
					"        for folder in oea.get_folders(oea.stage1np + '/xporter'):\r\n",
					"            print(folder)\r\n",
					"            if folder.isnumeric():\r\n",
					"                try:\r\n",
					"                    new_df = oea.load(folder,'groups.json',stage = oea.stage1np + '/xporter',data_format='json')\r\n",
					"                    new_df = new_df.select(F.explode('StudentMembers').alias('exploded_values')).select(\"exploded_values.*\")\r\n",
					"                    new_df = new_df.withColumn('SchoolID',lit(folder))            \r\n",
					"                    if df_StudentMembers is None:\r\n",
					"                        df_StudentMembers = new_df\r\n",
					"                    else:\r\n",
					"                        df_StudentMembers = df_StudentMembers.union(new_df)\r\n",
					"                except:\r\n",
					"                    pass\r\n",
					"        print('new_df3')\r\n",
					"        new_df.show()\r\n",
					"        df_StudentMembers.write.option(\"header\",\"true\").csv(oea.stage1np + '/xporter/StudentMemberscsv')\r\n",
					"        print('StudentMembers - ')\r\n",
					"        df_StudentMembers.show()\r\n",
					"\r\n",
					"    \r\n",
					"    def ingest_StudentMembers(self):\r\n",
					"        csvd = oea.load_csv(\"stage1np/xporter/StudentMemberscsv/*.csv\") #reading csv file\r\n",
					"        print(\"csvd\")\r\n",
					"        csvd.show()\r\n",
					"        oea.rm_if_exists(oea.stage1np + '/xporter/StudentMembers')\r\n",
					"        oea.land(\"xporter\", \"StudentMembers\", csvd)#creating batch folder\r\n",
					"        oea.ingest_snapshot_data('xporter', 'StudentMembers', xporter.schemas['StudentMembers'], 'SchoolID', primary_key='Id')#ingesting into stage 2\"\"\"\r\n",
					"\r\n",
					"\r\n",
					"    \r\n",
					"    def ingest_StudentMembers_stage3(self):\r\n",
					"        \"\"\" Processes delta batch data from stage2 into stage3 \"\"\"\r\n",
					"        source_path = f'{oea.stage2p}/xporter/StudentMembers_pseudo'\r\n",
					"        source_path2 = f'{oea.stage2np}/xporter/StudentMembers_lookup'\r\n",
					"        p_destination_path = f'{oea.stage3p}/xporter/StudentMembers_pseudo'\r\n",
					"        np_destination_path = f'{oea.stage3np}/xporter/StudentMembers_lookup'\r\n",
					"        spark_schema = oea.to_spark_schema(xporter.schemas['StudentMembers'])\r\n",
					"\r\n",
					"        df_1 = spark.read.load(source_path, format='parquet', header='true', schema=spark_schema)\r\n",
					"        df_2 = spark.read.load(source_path2, format='parquet', header='true', schema=spark_schema)\r\n",
					"        print(\"df_1\")\r\n",
					"        df_1.show()\r\n",
					"        print(\"df_2\")\r\n",
					"        df_2.show()\r\n",
					"        # df = df.dropDuplicates('EstabId') # More info: https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#streaming-deduplication\r\n",
					"        #df2 = df_1.withColumnRenamed('Name','School_Name').withColumnRenamed('Head', 'Head_Teacher')\r\n",
					"        df_pseudo = df_1\r\n",
					"        df_lookup = df_2\r\n",
					"        df_pseudo.write.save(p_destination_path, mode='overwrite', partitionBy='SchoolID')\r\n",
					"        df_lookup.write.save(np_destination_path, mode = 'overwrite', partitionBy = 'SchoolID')\r\n",
					"\r\n",
					"        #oea.ingest_snapshot_data_stage3('xporter', 'schoolinfo', xporter.schemas['schoolinfo'], 'SchoolID', primary_key='EstabId')\r\n",
					"        \r\n",
					"xporter = Xporter()"
				],
				"execution_count": 2
			}
		]
	}
}